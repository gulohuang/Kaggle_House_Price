{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7744a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe479e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311ee196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240305_215302\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240305_215302\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.3\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          24\n",
      "Memory Avail:       3.40 GB / 15.72 GB (21.7%)\n",
      "Disk Space Avail:   522.18 GB / 951.65 GB (54.9%)\n",
      "===================================================\n",
      "Train Data Rows:    1460\n",
      "Train Data Columns: 79\n",
      "Label Column:       SalePrice\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (755000, 34900, 180921.19589, 79442.50288)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3490.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
      "\t\t('int', [])    : 33 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\t\t('object', []) : 43 | ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 40 | ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', ...]\n",
      "\t\t('float', [])     :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
      "\t\t('int', [])       : 33 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['Street', 'Utilities', 'CentralAir']\n",
      "\t0.2s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.48 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1168, Val Rows: 292\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-51190.223\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-50517.5376\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 26908.7\n",
      "[2000]\tvalid_set's rmse: 26617.6\n",
      "[3000]\tvalid_set's rmse: 26569.8\n",
      "[4000]\tvalid_set's rmse: 26566.1\n",
      "[5000]\tvalid_set's rmse: 26569.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-26566.0786\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.1s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 29319.9\n",
      "[2000]\tvalid_set's rmse: 28847.1\n",
      "[3000]\tvalid_set's rmse: 28706.5\n",
      "[4000]\tvalid_set's rmse: 28663.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m Model_Autogluon \u001b[38;5;241m=\u001b[39m TabularPredictor(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(df_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mgargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1109\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m     ag_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_stack_levels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_stack_levels\n\u001b[0;32m   1107\u001b[0m     ag_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time_limit\n\u001b[1;32m-> 1109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(ag_fit_kwargs\u001b[38;5;241m=\u001b[39mag_fit_kwargs, ag_post_fit_kwargs\u001b[38;5;241m=\u001b[39mag_post_fit_kwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1115\u001b[0m, in \u001b[0;36mTabularPredictor._fit\u001b[1;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ag_fit_kwargs: \u001b[38;5;28mdict\u001b[39m, ag_post_fit_kwargs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_fit_kwargs)\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_post_fit_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:128\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 128\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    129\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    130\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    131\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    132\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    133\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    134\u001b[0m     holdout_frac\u001b[38;5;241m=\u001b[39mholdout_frac,\n\u001b[0;32m    135\u001b[0m     time_limit\u001b[38;5;241m=\u001b[39mtime_limit_trainer,\n\u001b[0;32m    136\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[0;32m    137\u001b[0m     infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    138\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_fit_kwargs,\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[0;32m    142\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:125\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m log_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, log_str)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi_and_ensemble(\n\u001b[0;32m    126\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    127\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    128\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    129\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    130\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    131\u001b[0m     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters,\n\u001b[0;32m    132\u001b[0m     num_stack_levels\u001b[38;5;241m=\u001b[39mnum_stack_levels,\n\u001b[0;32m    133\u001b[0m     time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m    134\u001b[0m     core_kwargs\u001b[38;5;241m=\u001b[39mcore_kwargs,\n\u001b[0;32m    135\u001b[0m     aux_kwargs\u001b[38;5;241m=\u001b[39maux_kwargs,\n\u001b[0;32m    136\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[0;32m    137\u001b[0m     infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    138\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    139\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2503\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_val)\n\u001b[0;32m   2502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_cols_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m-> 2503\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_multi_levels(\n\u001b[0;32m   2504\u001b[0m     X,\n\u001b[0;32m   2505\u001b[0m     y,\n\u001b[0;32m   2506\u001b[0m     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters,\n\u001b[0;32m   2507\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m   2508\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m   2509\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m   2510\u001b[0m     level_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   2511\u001b[0m     level_end\u001b[38;5;241m=\u001b[39mnum_stack_levels \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   2512\u001b[0m     time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m   2513\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2514\u001b[0m )\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2516\u001b[0m     \u001b[38;5;66;03m# TODO v1.0: Add toggle to raise exception if no models trained\u001b[39;00m\n\u001b[0;32m   2517\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: AutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:388\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    386\u001b[0m         core_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_core)\n\u001b[0;32m    387\u001b[0m         aux_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_aux)\n\u001b[1;32m--> 388\u001b[0m     base_model_names, aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_new_level(\n\u001b[0;32m    389\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    390\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    391\u001b[0m         X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    392\u001b[0m         y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    393\u001b[0m         X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    394\u001b[0m         models\u001b[38;5;241m=\u001b[39mhyperparameters,\n\u001b[0;32m    395\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    396\u001b[0m         base_model_names\u001b[38;5;241m=\u001b[39mbase_model_names,\n\u001b[0;32m    397\u001b[0m         core_kwargs\u001b[38;5;241m=\u001b[39mcore_kwargs_level,\n\u001b[0;32m    398\u001b[0m         aux_kwargs\u001b[38;5;241m=\u001b[39maux_kwargs_level,\n\u001b[0;32m    399\u001b[0m         name_suffix\u001b[38;5;241m=\u001b[39mname_suffix,\n\u001b[0;32m    400\u001b[0m         infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[0;32m    401\u001b[0m         infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    402\u001b[0m         full_weighted_ensemble\u001b[38;5;241m=\u001b[39mfull_weighted_ensemble,\n\u001b[0;32m    403\u001b[0m         additional_full_weighted_ensemble\u001b[38;5;241m=\u001b[39madditional_full_weighted_ensemble,\n\u001b[0;32m    404\u001b[0m     )\n\u001b[0;32m    405\u001b[0m     model_names_fit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base_model_names \u001b[38;5;241m+\u001b[39m aux_models\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:536\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size, full_weighted_ensemble, additional_full_weighted_ensemble)\u001b[0m\n\u001b[0;32m    534\u001b[0m     core_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[0;32m    535\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[1;32m--> 536\u001b[0m core_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_new_level_core(\n\u001b[0;32m    537\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    538\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    539\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    540\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    541\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    542\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m    543\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    544\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[0;32m    545\u001b[0m     infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    546\u001b[0m     base_model_names\u001b[38;5;241m=\u001b[39mbase_model_names,\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcore_kwargs,\n\u001b[0;32m    548\u001b[0m )\n\u001b[0;32m    550\u001b[0m aux_models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_weighted_ensemble:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:666\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi(\n\u001b[0;32m    667\u001b[0m     X\u001b[38;5;241m=\u001b[39mX_init,\n\u001b[0;32m    668\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    669\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    670\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    671\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    672\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m    673\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    674\u001b[0m     stack_name\u001b[38;5;241m=\u001b[39mstack_name,\n\u001b[0;32m    675\u001b[0m     compute_score\u001b[38;5;241m=\u001b[39mcompute_score,\n\u001b[0;32m    676\u001b[0m     fit_kwargs\u001b[38;5;241m=\u001b[39mfit_kwargs,\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    678\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2453\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2452\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 2453\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi_initial(\n\u001b[0;32m   2454\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   2455\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   2456\u001b[0m         models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m   2457\u001b[0m         k_fold\u001b[38;5;241m=\u001b[39mk_fold,\n\u001b[0;32m   2458\u001b[0m         n_repeats\u001b[38;5;241m=\u001b[39mn_repeats_initial,\n\u001b[0;32m   2459\u001b[0m         hyperparameter_tune_kwargs\u001b[38;5;241m=\u001b[39mhyperparameter_tune_kwargs,\n\u001b[0;32m   2460\u001b[0m         feature_prune_kwargs\u001b[38;5;241m=\u001b[39mfeature_prune_kwargs,\n\u001b[0;32m   2461\u001b[0m         time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m   2462\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2463\u001b[0m     )\n\u001b[0;32m   2464\u001b[0m     n_repeat_start \u001b[38;5;241m=\u001b[39m n_repeats_initial\n\u001b[0;32m   2465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2292\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bagged:\n\u001b[0;32m   2291\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2292\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi_fold(\n\u001b[0;32m   2293\u001b[0m         models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m   2294\u001b[0m         hyperparameter_tune_kwargs\u001b[38;5;241m=\u001b[39mhyperparameter_tune_kwargs,\n\u001b[0;32m   2295\u001b[0m         time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m   2296\u001b[0m         time_split\u001b[38;5;241m=\u001b[39mtime_split,\n\u001b[0;32m   2297\u001b[0m         time_ratio\u001b[38;5;241m=\u001b[39mtime_ratio,\n\u001b[0;32m   2298\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args,\n\u001b[0;32m   2299\u001b[0m     )\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2301\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2410\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2408\u001b[0m         time_start_model \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   2409\u001b[0m         time_left \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time_start_model \u001b[38;5;241m-\u001b[39m time_start)\n\u001b[1;32m-> 2410\u001b[0m model_name_trained_lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single_full(\n\u001b[0;32m   2411\u001b[0m     X, y, model, time_limit\u001b[38;5;241m=\u001b[39mtime_left, hyperparameter_tune_kwargs\u001b[38;5;241m=\u001b[39mhyperparameter_tune_kwargs_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   2412\u001b[0m )\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2183\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[0;32m   2179\u001b[0m         bagged_model_fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[0;32m   2180\u001b[0m             k_fold\u001b[38;5;241m=\u001b[39mk_fold, k_fold_start\u001b[38;5;241m=\u001b[39mk_fold_start, k_fold_end\u001b[38;5;241m=\u001b[39mk_fold_end, n_repeats\u001b[38;5;241m=\u001b[39mn_repeats, n_repeat_start\u001b[38;5;241m=\u001b[39mn_repeat_start\n\u001b[0;32m   2181\u001b[0m         )\n\u001b[0;32m   2182\u001b[0m         model_fit_kwargs\u001b[38;5;241m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[1;32m-> 2183\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_and_save(\n\u001b[0;32m   2184\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   2185\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   2186\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2187\u001b[0m         X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m   2188\u001b[0m         y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m   2189\u001b[0m         X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m   2190\u001b[0m         stack_name\u001b[38;5;241m=\u001b[39mstack_name,\n\u001b[0;32m   2191\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   2192\u001b[0m         compute_score\u001b[38;5;241m=\u001b[39mcompute_score,\n\u001b[0;32m   2193\u001b[0m         total_resources\u001b[38;5;241m=\u001b[39mtotal_resources,\n\u001b[0;32m   2194\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs,\n\u001b[0;32m   2195\u001b[0m     )\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   2197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1817\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1815\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single(X, y, model, X_val, y_val, total_resources\u001b[38;5;241m=\u001b[39mtotal_resources, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1819\u001b[0m fit_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_evaluation:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1763\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, total_resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AbstractModel:\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \u001b[38;5;124;03m    Returns trained model object.\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1763\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, X_val\u001b[38;5;241m=\u001b[39mX_val, y_val\u001b[38;5;241m=\u001b[39my_val, total_resources\u001b[38;5;241m=\u001b[39mtotal_resources, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:854\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_memory_usage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 854\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_model.py:218\u001b[0m, in \u001b[0;36mLGBModel._fit\u001b[1;34m(self, X, y, X_val, y_val, time_limit, num_gpus, num_cpus, sample_weight, sample_weight_val, verbosity, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_column in param dict is overridden.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m train_lgb_model(early_stopping_callback_kwargs\u001b[38;5;241m=\u001b[39mearly_stopping_callback_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_params)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LightGBMError:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\autogluon\\tabular\\models\\lgb\\lgb_utils.py:124\u001b[0m, in \u001b[0;36mtrain_lgb_model\u001b[1;34m(early_stopping_callback_kwargs, **train_params)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m booster\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_params)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lgb\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightgbm\\engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    269\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m    270\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    271\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    272\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[0;32m    273\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[0;32m    274\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 276\u001b[0m booster\u001b[38;5;241m.\u001b[39mupdate(fobj\u001b[38;5;241m=\u001b[39mfobj)\n\u001b[0;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightgbm\\basic.py:3658\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 3658\u001b[0m _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   3660\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(is_finished)))\n\u001b[0;32m   3661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#direct feed data to \n",
    "Model_Autogluon = TabularPredictor(label='SalePrice').fit(df_train.drop(columns=['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame(columns=[ 'Id','SalePrice'])\n",
    "submission['Id']=df_test['Id']\n",
    "submission['SalePrice']=Model_Autogluon.predict(df_test)\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "\n",
    "# Score: 0.13209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fff96322",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240227_154956\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240227_154956\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.3\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          24\n",
      "Memory Avail:       6.20 GB / 15.72 GB (39.4%)\n",
      "Disk Space Avail:   525.08 GB / 951.65 GB (55.2%)\n",
      "===================================================\n",
      "Train Data Rows:    1460\n",
      "Train Data Columns: 79\n",
      "Label Column:       SalePrice\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.534473028231162, 10.460242108190519, 12.02405, 0.39945)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6344.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
      "\t\t('int', [])    : 33 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\t\t('object', []) : 43 | ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 40 | ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', ...]\n",
      "\t\t('float', [])     :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
      "\t\t('int', [])       : 33 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['Street', 'Utilities', 'CentralAir']\n",
      "\t0.3s = Fit runtime\n",
      "\t79 features in original data used to generate 79 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.48 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.38s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1168, Val Rows: 292\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.2319\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.2289\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1265\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.68s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.12351\n",
      "[2000]\tvalid_set's rmse: 0.122908\n",
      "[3000]\tvalid_set's rmse: 0.122812\n",
      "[4000]\tvalid_set's rmse: 0.122805\n",
      "[5000]\tvalid_set's rmse: 0.122809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1228\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.56s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.1344\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.1314\t = Validation score   (-root_mean_squared_error)\n",
      "\t187.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.1343\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost ...\n",
      "\t-0.1278\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.95s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.1448\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.15s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.138638\n",
      "[2000]\tvalid_set's rmse: 0.138631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1386\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.97s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 0.486, 'XGBoost': 0.162, 'LightGBMXT': 0.135, 'NeuralNetTorch': 0.135, 'ExtraTreesMSE': 0.081}\n",
      "\t-0.1186\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 287.8s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240227_154956\")\n"
     ]
    }
   ],
   "source": [
    "#take the log of saleprice\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_train['SalePrice']=np.log(df_train['SalePrice'])\n",
    "Model_Autogluon = TabularPredictor(label='SalePrice').fit(df_train.drop(columns=['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f74ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Int features without null values at train time contain null values at inference time! Imputing nulls to 0. To avoid this, pass the features as floats during fit!\n",
      "WARNING: Int features with nulls: ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'GarageArea']\n"
     ]
    }
   ],
   "source": [
    "submission=pd.DataFrame(columns=[ 'Id','SalePrice'])\n",
    "submission['Id']=df_test['Id']\n",
    "submission['SalePrice']=np.exp(Model_Autogluon.predict(df_test))\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "# Score: 0.12614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f5c782",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guloo\\AppData\\Local\\Temp\\ipykernel_25216\\1305187673.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['SalePrice']=np.log(df_train['SalePrice'])\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240223_215104\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240223_215104\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.11.3\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          24\n",
      "Memory Avail:       5.59 GB / 15.72 GB (35.6%)\n",
      "Disk Space Avail:   526.95 GB / 951.65 GB (55.4%)\n",
      "===================================================\n",
      "Train Data Rows:    1460\n",
      "Train Data Columns: 330\n",
      "Label Column:       SalePrice\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.534473028231162, 10.460242108190519, 12.02405, 0.39945)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5726.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.81 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 267 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 27): ['MSZoning_nan', 'Street_nan', 'LotShape_nan', 'LandContour_nan', 'Utilities_nan', 'LotConfig_nan', 'LandSlope_nan', 'Neighborhood_nan', 'Condition1_nan', 'Condition2_nan', 'BldgType_nan', 'HouseStyle_nan', 'RoofStyle_nan', 'RoofMatl_nan', 'Exterior1st_nan', 'Exterior2nd_nan', 'ExterQual_nan', 'ExterCond_nan', 'Foundation_nan', 'Heating_nan', 'HeatingQC_nan', 'CentralAir_nan', 'KitchenQual_nan', 'Functional_nan', 'PavedDrive_nan', 'SaleType_nan', 'SaleCondition_nan']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 6): ['Exterior2nd_CBlock', 'BsmtCond_nan', 'BsmtFinType1_nan', 'GarageFinish_nan', 'GarageQual_nan', 'GarageCond_nan']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', []) : 6 | ['Exterior2nd_CBlock', 'BsmtCond_nan', 'BsmtFinType1_nan', 'GarageFinish_nan', 'GarageQual_nan', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])  : 261 | ['MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', ...]\n",
      "\t\t('float', []) :  11 | ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', ...]\n",
      "\t\t('int', [])   :  25 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  11 | ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', ...]\n",
      "\t\t('int', [])       :  25 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "\t\t('int', ['bool']) : 261 | ['MSZoning_C (all)', 'MSZoning_FV', 'MSZoning_RH', 'MSZoning_RL', 'MSZoning_RM', ...]\n",
      "\t1.3s = Fit runtime\n",
      "\t297 features in original data used to generate 297 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.76 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1168, Val Rows: 292\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.2319\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.2289\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.13189\n",
      "[2000]\tvalid_set's rmse: 0.130891\n",
      "[3000]\tvalid_set's rmse: 0.13068\n",
      "[4000]\tvalid_set's rmse: 0.130656\n",
      "[5000]\tvalid_set's rmse: 0.130664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1307\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.126657\n",
      "[2000]\tvalid_set's rmse: 0.125325\n",
      "[3000]\tvalid_set's rmse: 0.12515\n",
      "[4000]\tvalid_set's rmse: 0.12511\n",
      "[5000]\tvalid_set's rmse: 0.12511\n",
      "[6000]\tvalid_set's rmse: 0.125113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1251\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.137\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.1273\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.1362\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.0.0`. \n",
      "Fitting model: XGBoost ...\n",
      "\t-0.1395\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.1448\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.13578\n",
      "[2000]\tvalid_set's rmse: 0.135668\n",
      "[3000]\tvalid_set's rmse: 0.135666\n",
      "[4000]\tvalid_set's rmse: 0.135664\n",
      "[5000]\tvalid_set's rmse: 0.135663\n",
      "[6000]\tvalid_set's rmse: 0.135662\n",
      "[7000]\tvalid_set's rmse: 0.135662\n",
      "[8000]\tvalid_set's rmse: 0.135662\n",
      "[9000]\tvalid_set's rmse: 0.135662\n",
      "[10000]\tvalid_set's rmse: 0.135662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1357\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.26s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 0.5, 'CatBoost': 0.312, 'ExtraTreesMSE': 0.188}\n",
      "\t-0.1197\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 47.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240223_215104\")\n"
     ]
    }
   ],
   "source": [
    "#take the log of saleprice, one hot encoding\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_onehot=pd.get_dummies(pd.concat([df_train, df_test], axis=0), dummy_na=True).fillna(0)\n",
    "df_train=df_onehot[df_onehot['SalePrice']>0]\n",
    "df_test=df_onehot[df_onehot['SalePrice']==0].drop(['SalePrice'], axis=1)\n",
    "df_train['SalePrice']=np.log(df_train['SalePrice'])\n",
    "Model_Autogluon = TabularPredictor(label='SalePrice').fit(df_train.drop(columns=['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f88f2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame(columns=[ 'Id','SalePrice'])\n",
    "submission['Id']=df_test['Id']\n",
    "submission['SalePrice']=np.exp(Model_Autogluon.predict(df_test))\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "# Score: 0.1497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3177bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Autogluon=TabularPredictor.load('AutogluonModels/ag-20240227_154956')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d9bcf8e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val              eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2  -0.118579  root_mean_squared_error       0.150856   51.122861                0.001136           0.163970            2       True         11\n",
      "1              LightGBM  -0.122804  root_mean_squared_error       0.031251   29.560527                0.031251          29.560527            1       True          4\n",
      "2            LightGBMXT  -0.126542  root_mean_squared_error       0.021333   10.681813                0.021333          10.681813            1       True          3\n",
      "3               XGBoost  -0.127760  root_mean_squared_error       0.016705    1.948824                0.016705           1.948824            1       True          8\n",
      "4              CatBoost  -0.131422  root_mean_squared_error       0.017649  187.005504                0.017649         187.005504            1       True          6\n",
      "5         ExtraTreesMSE  -0.134263  root_mean_squared_error       0.048876    0.616309                0.048876           0.616309            1       True          7\n",
      "6       RandomForestMSE  -0.134363  root_mean_squared_error       0.046876    0.953321                0.046876           0.953321            1       True          5\n",
      "7         LightGBMLarge  -0.138630  root_mean_squared_error       0.032553   46.966744                0.032553          46.966744            1       True         10\n",
      "8        NeuralNetTorch  -0.144759  root_mean_squared_error       0.031555    8.151418                0.031555           8.151418            1       True          9\n",
      "9        KNeighborsDist  -0.228925  root_mean_squared_error       0.005249    0.007498                0.005249           0.007498            1       True          2\n",
      "10       KNeighborsUnif  -0.231873  root_mean_squared_error       0.018209    0.043616                0.018209           0.043616            1       True          1\n",
      "Number of models trained: 11\n",
      "Types of models trained:\n",
      "{'TabularNeuralNetTorchModel', 'XGBoostModel', 'LGBModel', 'WeightedEnsembleModel', 'CatBoostModel', 'KNNModel', 'XTModel', 'RFModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  : 40 | ['MSZoning', 'Alley', 'LotShape', 'LandContour', 'LotConfig', ...]\n",
      "('float', [])     :  3 | ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\n",
      "('int', [])       : 33 | ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', ...]\n",
      "('int', ['bool']) :  3 | ['Street', 'Utilities', 'CentralAir']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20240227_154956SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif': 'KNNModel',\n",
       "  'KNeighborsDist': 'KNNModel',\n",
       "  'LightGBMXT': 'LGBModel',\n",
       "  'LightGBM': 'LGBModel',\n",
       "  'RandomForestMSE': 'RFModel',\n",
       "  'CatBoost': 'CatBoostModel',\n",
       "  'ExtraTreesMSE': 'XTModel',\n",
       "  'XGBoost': 'XGBoostModel',\n",
       "  'NeuralNetTorch': 'TabularNeuralNetTorchModel',\n",
       "  'LightGBMLarge': 'LGBModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif': -0.23187279197689914,\n",
       "  'KNeighborsDist': -0.22892488005197162,\n",
       "  'LightGBMXT': -0.12654170233227985,\n",
       "  'LightGBM': -0.12280410490145952,\n",
       "  'RandomForestMSE': -0.13436267977197214,\n",
       "  'CatBoost': -0.13142209558008425,\n",
       "  'ExtraTreesMSE': -0.13426333061546492,\n",
       "  'XGBoost': -0.1277598711724209,\n",
       "  'NeuralNetTorch': -0.14475927301308616,\n",
       "  'LightGBMLarge': -0.13863041509194432,\n",
       "  'WeightedEnsemble_L2': -0.11857943154519582},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif': ['KNeighborsUnif'],\n",
       "  'KNeighborsDist': ['KNeighborsDist'],\n",
       "  'LightGBMXT': ['LightGBMXT'],\n",
       "  'LightGBM': ['LightGBM'],\n",
       "  'RandomForestMSE': ['RandomForestMSE'],\n",
       "  'CatBoost': ['CatBoost'],\n",
       "  'ExtraTreesMSE': ['ExtraTreesMSE'],\n",
       "  'XGBoost': ['XGBoost'],\n",
       "  'NeuralNetTorch': ['NeuralNetTorch'],\n",
       "  'LightGBMLarge': ['LightGBMLarge'],\n",
       "  'WeightedEnsemble_L2': ['WeightedEnsemble_L2']},\n",
       " 'model_fit_times': {'KNeighborsUnif': 0.04361557960510254,\n",
       "  'KNeighborsDist': 0.007497549057006836,\n",
       "  'LightGBMXT': 10.68181300163269,\n",
       "  'LightGBM': 29.560526609420776,\n",
       "  'RandomForestMSE': 0.9533207416534424,\n",
       "  'CatBoost': 187.00550389289856,\n",
       "  'ExtraTreesMSE': 0.6163091659545898,\n",
       "  'XGBoost': 1.948824405670166,\n",
       "  'NeuralNetTorch': 8.15141773223877,\n",
       "  'LightGBMLarge': 46.96674418449402,\n",
       "  'WeightedEnsemble_L2': 0.1639697551727295},\n",
       " 'model_pred_times': {'KNeighborsUnif': 0.018209218978881836,\n",
       "  'KNeighborsDist': 0.0052492618560791016,\n",
       "  'LightGBMXT': 0.021332740783691406,\n",
       "  'LightGBM': 0.031250953674316406,\n",
       "  'RandomForestMSE': 0.046875715255737305,\n",
       "  'CatBoost': 0.017648696899414062,\n",
       "  'ExtraTreesMSE': 0.04887580871582031,\n",
       "  'XGBoost': 0.016704559326171875,\n",
       "  'NeuralNetTorch': 0.0315554141998291,\n",
       "  'LightGBMLarge': 0.03255295753479004,\n",
       "  'WeightedEnsemble_L2': 0.0011360645294189453},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'model_hyperparams': {'KNeighborsUnif': {'weights': 'uniform'},\n",
       "  'KNeighborsDist': {'weights': 'distance'},\n",
       "  'LightGBMXT': {'learning_rate': 0.05, 'extra_trees': True},\n",
       "  'LightGBM': {'learning_rate': 0.05},\n",
       "  'RandomForestMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'CatBoost': {'iterations': 10000,\n",
       "   'learning_rate': 0.05,\n",
       "   'random_seed': 0,\n",
       "   'allow_writing_files': False,\n",
       "   'eval_metric': 'RMSE'},\n",
       "  'ExtraTreesMSE': {'n_estimators': 300,\n",
       "   'max_leaf_nodes': 15000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'bootstrap': True,\n",
       "   'criterion': 'squared_error'},\n",
       "  'XGBoost': {'n_estimators': 10000,\n",
       "   'learning_rate': 0.1,\n",
       "   'n_jobs': -1,\n",
       "   'proc.max_category_levels': 100,\n",
       "   'objective': 'reg:squarederror',\n",
       "   'booster': 'gbtree'},\n",
       "  'NeuralNetTorch': {'num_epochs': 500,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0003,\n",
       "   'weight_decay': 1e-06,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'LightGBMLarge': {'learning_rate': 0.03,\n",
       "   'num_leaves': 128,\n",
       "   'feature_fraction': 0.9,\n",
       "   'min_data_in_leaf': 5},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                   model  score_val              eval_metric  pred_time_val  \\\n",
       " 0   WeightedEnsemble_L2  -0.118579  root_mean_squared_error       0.150856   \n",
       " 1              LightGBM  -0.122804  root_mean_squared_error       0.031251   \n",
       " 2            LightGBMXT  -0.126542  root_mean_squared_error       0.021333   \n",
       " 3               XGBoost  -0.127760  root_mean_squared_error       0.016705   \n",
       " 4              CatBoost  -0.131422  root_mean_squared_error       0.017649   \n",
       " 5         ExtraTreesMSE  -0.134263  root_mean_squared_error       0.048876   \n",
       " 6       RandomForestMSE  -0.134363  root_mean_squared_error       0.046876   \n",
       " 7         LightGBMLarge  -0.138630  root_mean_squared_error       0.032553   \n",
       " 8        NeuralNetTorch  -0.144759  root_mean_squared_error       0.031555   \n",
       " 9        KNeighborsDist  -0.228925  root_mean_squared_error       0.005249   \n",
       " 10       KNeighborsUnif  -0.231873  root_mean_squared_error       0.018209   \n",
       " \n",
       "       fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       " 0    51.122861                0.001136           0.163970            2   \n",
       " 1    29.560527                0.031251          29.560527            1   \n",
       " 2    10.681813                0.021333          10.681813            1   \n",
       " 3     1.948824                0.016705           1.948824            1   \n",
       " 4   187.005504                0.017649         187.005504            1   \n",
       " 5     0.616309                0.048876           0.616309            1   \n",
       " 6     0.953321                0.046876           0.953321            1   \n",
       " 7    46.966744                0.032553          46.966744            1   \n",
       " 8     8.151418                0.031555           8.151418            1   \n",
       " 9     0.007498                0.005249           0.007498            1   \n",
       " 10    0.043616                0.018209           0.043616            1   \n",
       " \n",
       "     can_infer  fit_order  \n",
       " 0        True         11  \n",
       " 1        True          4  \n",
       " 2        True          3  \n",
       " 3        True          8  \n",
       " 4        True          6  \n",
       " 5        True          7  \n",
       " 6        True          5  \n",
       " 7        True         10  \n",
       " 8        True          9  \n",
       " 9        True          2  \n",
       " 10       True          1  }"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Autogluon.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d09474",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Autogluon=TabularPredictor.load('AutogluonModels/ag-20240227_154956')\n",
    "\n",
    "# Model_Autogluon.leaderboard(df_test)\n",
    "# Model_Autogluon.evaluate(df_test)\n",
    "# Model_Autogluon.feature_importance(test_data)\n",
    "# Model_Autogluon.predict(test_data, model='LightGBM')\n",
    "# Model_Autogluon.model_names()\n",
    "# Model_Autogluon.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c0ad7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 79 features using 1460 rows with 5 shuffle sets...\n",
      "\t194.97s\t= Expected runtime (38.99s per shuffle set)\n",
      "\t81.63s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    }
   ],
   "source": [
    "imp=Model_Autogluon.feature_importance(df_train.drop(columns=['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10592ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.to_csv('output.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
